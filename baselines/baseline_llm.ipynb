{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "# import spacy\n",
    "from spacy import load\n",
    "# import spacy.cli\n",
    "\n",
    "# spacy.cli.download(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_PATH = os.path.abspath(\"../config.ini\")\n",
    "os.path.isfile(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser()\n",
    "config.read(CONFIG_PATH)\n",
    "\n",
    "llm_vendors = config[\"llm_vendors\"]\n",
    "env_file = Path(config[\"env\"][\"env_file\"]).as_posix()\n",
    "\n",
    "\n",
    "load_dotenv(env_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(os.path.abspath(\"../data/raw\")).as_posix()\n",
    "PROCESSED_DATA_PATH = Path(os.path.abspath(\"../data/processed\")).as_posix()\n",
    "EXTERNAL_PATH = Path(os.path.abspath(\"../data/external\")).as_posix()\n",
    "INTERIM_PATH = Path(os.path.abspath(\"../data/interim\")).as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(BaseModel):\n",
    "    \"\"\"Модель для представления сообщения в диалоге\n",
    "\n",
    "    Attributes:\n",
    "        role (str): Роль отправителя сообщения (system, user или assistant)\n",
    "        content (str): Текст сообщения\n",
    "    \"\"\"\n",
    "\n",
    "    role: str = Field(description=\"Роль в диалоге (system, user, assistant)\")\n",
    "    content: str = Field(description=\"Содержание сообщения\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сильно хуже!\n",
    "\n",
    "# class Company(BaseModel):\n",
    "#     name: Optional[str] = Field(description=\"Company name\")\n",
    "\n",
    "\n",
    "class Companies(BaseModel):\n",
    "    names: Optional[list[str]] = Field(description=\"Companies names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_service_models(url: str, api_key: str) -> list:\n",
    "    \"\"\"Просмотр доступных моделей у поставщика\n",
    "\n",
    "    Args:\n",
    "        url (str): URL-адрес поставщика\n",
    "        key_name (str): название API-ключа\n",
    "\n",
    "    Returns:\n",
    "        list: список доступных моделей\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{url}models\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {api_key}\",\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        models = response.json().get(\"data\", [])\n",
    "        return [model[\"id\"] for model in models if \"id\" in model]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка при получении списка моделей: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion_parse(\n",
    "    base_url: str,\n",
    "    api_key: str,\n",
    "    model: str,\n",
    "    temperature: float = 0.3,\n",
    "    messages: Optional[list[Prompt]] = None,\n",
    "    max_tokens: int = 4096,\n",
    ") -> str:\n",
    "    \"\"\"Выполняет запрос к языковой модели\n",
    "\n",
    "    Args:\n",
    "        url (str): базовый URL API\n",
    "        api_key (str): ключ API\n",
    "        model (str): идентификатор модели\n",
    "        temperature (float, optional): параметр температуры. Defaults to 0.5.\n",
    "        messages (Optional[list[Prompt]], optional): список сообщений. Defaults to None.\n",
    "        max_tokens (int, optional): максимальное количество токенов. Defaults to 1500.\n",
    "\n",
    "    Returns:\n",
    "        str: ответ модели\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_format=Companies,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            frequency_penalty=0.9,\n",
    "        )\n",
    "        event = completion.choices[0].message.parsed\n",
    "        return event.names\n",
    "    except Exception:\n",
    "        return []\n",
    "    # except pydantic.ValidationError as e:\n",
    "    #     return []\n",
    "    # except AttributeError as e:\n",
    "    #     return []\n",
    "    # except TypeError as e:\n",
    "    #     return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from openai import RateLimitError\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def llm_ner(\n",
    "    base_url=\"http://127.0.0.1:11434/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"deepseek-r1-distill-qwen-32b\",\n",
    "    texts: Optional[list[str]] = None,\n",
    "):\n",
    "    system_prompt = Prompt(\n",
    "        role=\"system\",\n",
    "        content=\"Instructions:\"\n",
    "        \"You are given one or more user review texts. Extract all unique company names (brands, organizations, firms) and output a valid JSON following the Company schema.\"\n",
    "        'If no companies are found, \"names\" should be an empty list.'\n",
    "        \"\"\n",
    "        \"Output Format:\"\n",
    "        \"{\"\n",
    "        '\"names\": [\"Company1\", \"Company2\", ...]'\n",
    "        \"}\"\n",
    "        \"\"\n",
    "        \"Example:\"\n",
    "        \"Input: \\\"I visited 'GastroMania' restaurant and got a gadget from 'TechWorld'.\\\"\"\n",
    "        \"Output:\"\n",
    "        \"{\"\n",
    "        '\"names\": [\"GastroMania\", \"TechWorld\"]'\n",
    "        \"}\"\n",
    "        \"\"\n",
    "        \"Do not include any extra text. Begin processing with the following review:\",\n",
    "    )\n",
    "    names = []\n",
    "    for text in tqdm(texts):\n",
    "        try:\n",
    "            sleep(1)\n",
    "            feedback = Prompt(role=\"user\", content=text)\n",
    "            llm_companies = get_chat_completion_parse(\n",
    "                base_url=base_url,\n",
    "                api_key=api_key,\n",
    "                model=model,\n",
    "                messages=[system_prompt, feedback],\n",
    "            )\n",
    "            # print(llm_companies)\n",
    "            names.append(llm_companies)\n",
    "        except RateLimitError:\n",
    "            names.append([])\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(sheet_path):\n",
    "    if sheet_path.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(sheet_path)\n",
    "    elif sheet_path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(sheet_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    df.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], inplace=True)\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"отзывы (2-100)посты(100-150) НОВОСТЬ (ТЕКСТ)\": \"feedback\",\n",
    "            \"Название компании упоминаемой\": \"companies_name\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df.fillna({\"companies_name\": \"---\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ner_results(ner_companies: list[list[str]], model_name: str):\n",
    "    result = pd.DataFrame(\n",
    "        [\", \".join(comp) for comp in ner_companies], columns=[\"llm_companies\"]\n",
    "    )\n",
    "    saved_path = Path(INTERIM_PATH) / f\"{model_name}.csv\"\n",
    "    result.to_csv(saved_path, index=False)\n",
    "\n",
    "\n",
    "def load_ner_results(model_name: str):\n",
    "    load_path = Path(INTERIM_PATH) / f\"{model_name}.csv\"\n",
    "    df = pd.read_csv(load_path)\n",
    "    df.fillna({\"llm_companies\": \"---\"}, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_results(companies: list[list[str]], model_name: str, method: str):\n",
    "    joined = [\" \".join(comps) for comps in companies]\n",
    "    result = pd.DataFrame(joined, columns=[\"llm_companies\"])\n",
    "    saved_path = Path(INTERIM_PATH) / f\"{model_name}_{method}_cleaned.csv\"\n",
    "    result.to_csv(saved_path, index=False)\n",
    "    if os.path.isfile(saved_path):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def load_cleaned_results(model_name: str, method: str) -> list[list[str]]:\n",
    "    saved_path = Path(INTERIM_PATH) / f\"{model_name}_{method}_cleaned.csv\"\n",
    "    comps_df = pd.read_csv(saved_path)\n",
    "    result = [comps[0].split() for comps in comps_df.values.tolist()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lemmatized(lemmatized_companies: list[list[str]]) -> list[list[str]]:\n",
    "    cleaned_companies = [\n",
    "        [comp.strip(\"\\n\") for comp in comps if comp.isalnum() or comp == \"---\\n\"]\n",
    "        for comps in lemmatized_companies\n",
    "    ]\n",
    "    result = [[\"---\"] if comp == [] else comp for comp in cleaned_companies]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lem_companies(feedbacks: list[list[str]]):\n",
    "    load_model = load(\"ru_core_news_sm\")\n",
    "    lemmas = []\n",
    "    for companies in feedbacks:\n",
    "        lemma = []\n",
    "        for company in load_model.pipe(companies):\n",
    "            lemma.append([n.lemma_ for n in company])\n",
    "        lemmas.append(lemma[0])\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lem_companies(feedbacks: list[list[str]]):\n",
    "    stem = Mystem()\n",
    "    result = []\n",
    "    companies_joined = list(map(lambda x: \" \".join(x), feedbacks))\n",
    "    for companies in companies_joined:\n",
    "        result.append(stem.lemmatize(companies))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(doc1, doc2):\n",
    "    words_doc1 = set(doc1)\n",
    "    words_doc2 = set(doc2)\n",
    "    intersection = words_doc1.intersection(words_doc2)\n",
    "    union = words_doc1.union(words_doc2)\n",
    "    return float(len(intersection)) / len(union)\n",
    "\n",
    "\n",
    "def jaccard_feedbacks(ner_llm: list[list[str]], labels: list[list[str]]) -> float:\n",
    "    assert len(ner_llm) == len(labels), \"Длины списков должны совпадать\"\n",
    "    scores = []\n",
    "    for pred, label in zip(ner_llm, labels):\n",
    "        scores.append(jaccard_similarity(pred, label))\n",
    "    jaccard_metric = np.round(np.mean(scores), 2)\n",
    "    return jaccard_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jaccard_results(llm_jaccard) -> None:\n",
    "    with open(Path(PROCESSED_DATA_PATH) / \"llm_jaccard.csv\", \"a\") as f:\n",
    "        for model, jaccard in llm_jaccard.items():\n",
    "            f.write(f\"{model},{jaccard}\\n\")\n",
    "    status = os.path.isfile(Path(PROCESSED_DATA_PATH) / \"llm_jaccard.csv\")\n",
    "    return status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Потому что абхазские мандарины на фуд сити или рынках, а не в пятёрочке 😂 \\r\\nЯ вообще после Краснодара удивляюсь, как люди в Москве покупают зелёную хурму например)) \\r\\nНо для меня, самые вкусны мандарины – Марокко\\r',\n",
       " 'Последняя в коллабе с пивоварней питерской Газ Брю Крутые ребята Я к ним ходил в бар-пивоварню на экскурсию \\nСоветую для приезжих и живущих\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prepare_dataframe(os.path.join(DATA_PATH, \"NewsList_1.csv\"))\n",
    "texts = df[\"feedback\"].tolist()\n",
    "lowered_labels = [comps.lower() for comps in df[\"companies_name\"].values.tolist()]\n",
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_no_lemma = [comps.split(\", \") for comps in lowered_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_jaccard = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [11:17<00:00,  4.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# llm_companies = llm_ner(base_url=llm_vendors['lm-studio'],\n",
    "#                         api_key=os.getenv('LM_STUDIO'),\n",
    "#                         model=model,\n",
    "#                         texts=texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_ner_results(llm_companies, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_results = load_ner_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "# ner_lemm_ = clean_lemmatized(ner_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_jaccard[model] = jaccard_metric.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3b-latest': np.float64(0.43), 'gemma-2-2b-it': np.float64(0.28)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3b_results_lemm_stem = stem_lem_companies(h3b_results.values.tolist())\n",
    "h3b_results_lemm_stem_ = clean_lemmatized(h3b_results_lemm_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert save_cleaned_results(h3b_results_lemm_stem_, \"hermes-3-llama-3.2-3b\", \"stem\"), (\n",
    "    \"File is not saved!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    }
   ],
   "source": [
    "h3b_results_lemm_stem__ = load_cleaned_results(\"hermes-3-llama-3.2-3b\", \"stem\")\n",
    "\n",
    "jaccard_h3b_stem = []\n",
    "\n",
    "assert len(h3b_results_lemm_stem__) == len(labels_no_lemma), (\n",
    "    \"Lengths are the different!\"\n",
    ")\n",
    "\n",
    "for i, comps in enumerate(h3b_results_lemm_stem__):\n",
    "    jaccard_h3b_stem.append(jaccard_similarity(comps, labels_no_lemma[i]))\n",
    "\n",
    "jaccard_h3b_stem_metrics = np.mean(jaccard_h3b_stem)\n",
    "print(f\"{jaccard_h3b_stem_metrics:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ministral-3b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"ministral-3b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_companies = llm_ner(base_url=llm_vendors['lm-studio'],\n",
    "#                         api_key=os.getenv('LM_STUDIO'),\n",
    "#                         model=model,\n",
    "#                         texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_ner_results(llm_companies, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_results = load_ner_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "# ner_lemm_ = clean_lemmatized(ner_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_jaccard[model] = jaccard_metric.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_jaccard[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3b-latest': np.float64(0.43),\n",
       " 'gemma-2-2b-it': np.float64(0.28),\n",
       " 'ministral-3b-instruct': np.float64(0.0)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deepseek-r1-distill-qwen-1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"deepseek-r1-distill-qwen-1.5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_companies = llm_ner(base_url=llm_vendors['lm-studio'],\n",
    "#                         api_key=os.getenv('LM_STUDIO'),\n",
    "#                         model=model,\n",
    "#                         texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_ner_results(llm_companies, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_results = load_ner_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "# ner_lemm_ = clean_lemmatized(ner_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_jaccard[model] = jaccard_metric.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3b-latest': np.float64(0.43),\n",
       " 'gemma-2-2b-it': np.float64(0.28),\n",
       " 'ministral-3b-instruct': np.float64(0.0),\n",
       " 'deepseek-r1-distill-qwen-1.5b': np.float64(0.13)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Small**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ministral-3b-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"ministral-3b-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_companies = llm_ner(base_url=llm_vendors['mistral'],\n",
    "#                         api_key=os.getenv('MISTRAL_API_KEY'),\n",
    "#                         model=model,\n",
    "#                         texts=texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_ner_results(llm_companies, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m3b_results = load_ner_results('ministral-3b-latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m3b_results_lemm_spacy = spacy_lem_companies(m3b_results.values.tolist())\n",
    "# m3b_results_lemm_spacy_ = clean_lemmatized(m3b_results_lemm_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert save_cleaned_results(m3b_results_lemm_spacy_, 'ministral-3b-latest', \"spacy\"), \"File is not saved!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.43)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "jaccard_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_jaccard[model] = jaccard_metric.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3b-latest': np.float64(0.43)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3b_results_lemm_stem = stem_lem_companies(m3b_results.values.tolist())\n",
    "m3b_results_lemm_stem_ = clean_lemmatized(m3b_results_lemm_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert save_cleaned_results(m3b_results_lemm_stem_, \"ministral-3b-latest\", \"stem\"), (\n",
    "    \"File is not saved!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3b_results_lemm_stem__ = load_cleaned_results(\"ministral-3b-latest\", \"stem\")\n",
    "\n",
    "jaccard_m3b_stem = []\n",
    "\n",
    "assert len(m3b_results_lemm_stem__) == len(labels_no_lemma), (\n",
    "    \"Lengths are the different!\"\n",
    ")\n",
    "\n",
    "for i, comps in enumerate(m3b_results_lemm_stem__):\n",
    "    jaccard_m3b_stem.append(jaccard_similarity(comps, labels_no_lemma[i]))\n",
    "\n",
    "jaccard_m3b_stem_metrics = np.mean(jaccard_m3b_stem)\n",
    "print(f\"{jaccard_m3b_stem_metrics:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['claude-3-opus',\n",
       " 'claude-3.5-haiku',\n",
       " 'claude-3.5-haiku-20241022',\n",
       " 'claude-3.5-sonnet',\n",
       " 'claude-3.5-sonnet-20240620',\n",
       " 'codestral-2501',\n",
       " 'codestral-mamba',\n",
       " 'command-r-plus-08-2024',\n",
       " 'dall-e-3',\n",
       " 'dbrx-instruct',\n",
       " 'deepseek-chat',\n",
       " 'deepseek-chat-v2.5',\n",
       " 'deepseek-r1',\n",
       " 'deepseek-r1-distill-llama-70b',\n",
       " 'deepseek-r1-distill-qwen-14b',\n",
       " 'deepseek-r1-distill-qwen-32b',\n",
       " 'dolphin-mixtral-8x7b',\n",
       " 'eva-llama-3.33-70b',\n",
       " 'eva-qwen-2.5-32b',\n",
       " 'eva-qwen-2.5-72b',\n",
       " 'fimbulvetr-11b-v2',\n",
       " 'flux',\n",
       " 'flux-1.1-pro',\n",
       " 'flux-1.1-pro-ultra',\n",
       " 'flux-dev',\n",
       " 'flux-dev-lora',\n",
       " 'flux-dev-multi-lora',\n",
       " 'flux-fill-dev',\n",
       " 'flux-fill-pro',\n",
       " 'flux-pro',\n",
       " 'flux-pulid',\n",
       " 'flux-schnell',\n",
       " 'flux-schnell-lora',\n",
       " 'gemini-2.0-flash-001',\n",
       " 'gemini-2.0-flash-exp:free',\n",
       " 'gemini-2.0-flash-lite-preview-02-05:free',\n",
       " 'gemini-2.0-flash-thinking-exp-1219:free',\n",
       " 'gemini-2.0-flash-thinking-exp:free',\n",
       " 'gemini-2.0-pro-exp-02-05:free',\n",
       " 'gemini-exp-1206:free',\n",
       " 'gemini-flash-1.5',\n",
       " 'gemini-flash-1.5-8b',\n",
       " 'gemini-flash-1.5-8b-exp',\n",
       " 'gemini-pro-1.5',\n",
       " 'gemma-2-27b-it',\n",
       " 'gemma-2-9b-it',\n",
       " 'gemma-2-9b-it:free',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-4',\n",
       " 'gpt-4-0314',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4-32k',\n",
       " 'gpt-4-32k-0314',\n",
       " 'gpt-4-turbo',\n",
       " 'gpt-4-turbo-preview',\n",
       " 'gpt-4o',\n",
       " 'gpt-4o-2024-05-13',\n",
       " 'gpt-4o-2024-08-06',\n",
       " 'gpt-4o-2024-11-20',\n",
       " 'gpt-4o-mini',\n",
       " 'gpt-4o-mini-2024-07-18',\n",
       " 'gpt-4o:extended',\n",
       " 'grok-beta',\n",
       " 'hermes-2-pro-llama-3-8b',\n",
       " 'hermes-3-llama-3.1-405b',\n",
       " 'hermes-3-llama-3.1-70b',\n",
       " 'hyper-flux-16step',\n",
       " 'hyper-flux-8step',\n",
       " 'inflection-3-pi',\n",
       " 'inflection-3-productivity',\n",
       " 'jamba-1-5-large',\n",
       " 'jamba-1-5-mini',\n",
       " 'jamba-instruct',\n",
       " 'l3-euryale-70b',\n",
       " 'l3-lunaris-8b',\n",
       " 'l3.1-70b-hanami-x1',\n",
       " 'l3.1-euryale-70b',\n",
       " 'l3.3-euryale-70b',\n",
       " 'learnlm-1.5-pro-experimental:free',\n",
       " 'lfm-3b',\n",
       " 'lfm-40b',\n",
       " 'lfm-7b',\n",
       " 'llama-3.1-405b',\n",
       " 'llama-3.1-405b-instruct',\n",
       " 'llama-3.1-70b-instruct',\n",
       " 'llama-3.1-8b-instruct',\n",
       " 'llama-3.1-lumimaid-70b',\n",
       " 'llama-3.1-lumimaid-8b',\n",
       " 'llama-3.1-nemotron-70b-instruct',\n",
       " 'llama-3.1-sonar-huge-128k-online',\n",
       " 'llama-3.1-sonar-large-128k-chat',\n",
       " 'llama-3.1-sonar-large-128k-online',\n",
       " 'llama-3.1-sonar-small-128k-chat',\n",
       " 'llama-3.1-sonar-small-128k-online',\n",
       " 'llama-3.2-11b-vision-instruct',\n",
       " 'llama-3.2-11b-vision-instruct:free',\n",
       " 'llama-3.2-1b-instruct',\n",
       " 'llama-3.2-3b-instruct',\n",
       " 'llama-3.2-90b-vision-instruct',\n",
       " 'llama-3.3-70b-instruct',\n",
       " 'llama-guard-3-8b',\n",
       " 'magnum-v2-72b',\n",
       " 'magnum-v4-72b',\n",
       " 'minimax-01',\n",
       " 'ministral-3b',\n",
       " 'ministral-8b',\n",
       " 'mistral-7b-instruct',\n",
       " 'mistral-7b-instruct-v0.3',\n",
       " 'mistral-large',\n",
       " 'mistral-large-2407',\n",
       " 'mistral-large-2411',\n",
       " 'mistral-nemo',\n",
       " 'mistral-saba',\n",
       " 'mistral-small-24b-instruct-2501',\n",
       " 'mixtral-8x22b-instruct',\n",
       " 'mixtral-8x7b',\n",
       " 'mixtral-8x7b-instruct',\n",
       " 'mn-inferor-12b',\n",
       " 'mythalion-13b',\n",
       " 'mythomax-l2-13b',\n",
       " 'mythomax-l2-13b:free',\n",
       " 'noromaid-20b',\n",
       " 'o1',\n",
       " 'o1-mini',\n",
       " 'o1-mini-2024-09-12',\n",
       " 'o1-preview-2024-09-12',\n",
       " 'o3-mini',\n",
       " 'o3-mini-high',\n",
       " 'openchat-7b',\n",
       " 'openchat-7b:free',\n",
       " 'palm-2-chat-bison',\n",
       " 'palm-2-chat-bison-32k',\n",
       " 'palm-2-codechat-bison',\n",
       " 'palm-2-codechat-bison-32k',\n",
       " 'phi-3-medium-128k-instruct',\n",
       " 'phi-3-medium-128k-instruct:free',\n",
       " 'phi-3-mini-128k-instruct',\n",
       " 'phi-3-mini-128k-instruct:free',\n",
       " 'phi-3.5-mini-128k-instruct',\n",
       " 'pixtral-12b',\n",
       " 'pixtral-large-2411',\n",
       " 'qwen-2-72b-instruct',\n",
       " 'qwen-2-vl-72b-instruct',\n",
       " 'qwen-2-vl-7b-instruct',\n",
       " 'qwen-2.5-72b-instruct',\n",
       " 'qwen-2.5-7b-instruct',\n",
       " 'qwen-2.5-coder-32b-instruct',\n",
       " 'qwen-vl-plus:free',\n",
       " 'qwq-32b-preview',\n",
       " 'rocinante-12b',\n",
       " 'rogue-rose-103b-v0.2:free',\n",
       " 'sonar',\n",
       " 'sonar-reasoning',\n",
       " 'stable-diffusion-3',\n",
       " 'text-embedding-3-large',\n",
       " 'text-embedding-3-small',\n",
       " 'text-embedding-ada-002',\n",
       " 'toppy-m-7b',\n",
       " 'toppy-m-7b:free',\n",
       " 'tts-1',\n",
       " 'tts-1-1106',\n",
       " 'tts-1-hd',\n",
       " 'tts-1-hd-1106',\n",
       " 'unslopnemo-12b',\n",
       " 'vetkastar',\n",
       " 'weaver',\n",
       " 'whisper-1',\n",
       " 'xwin-lm-70b',\n",
       " 'yi-large']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = requests.get(\n",
    "    \"https://bothub.chat/api/v2/model/list?children=1\",\n",
    "    headers={\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": os.getenv(\"BOT_HUB_KEY\"),\n",
    "    },\n",
    ").json()\n",
    "\n",
    "sorted([model.get(\"id\") for model in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# НЕ РАБОТАЕТ!\n",
    "model = \"llama-3.1-8b-instruct\"\n",
    "# llm_companies = llm_ner(base_url=llm_vendors['bothub'],\n",
    "#                         api_key=os.getenv('BOT_HUB_KEY'),\n",
    "#                         model=model,\n",
    "#                         texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_ner_results(llm_companies, model)\n",
    "# llm_results = load_ner_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_lemm = spacy_lem_companies(llm_results.values.tolist())\n",
    "# ner_lemm_ = clean_lemmatized(ner_lemm)\n",
    "# assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.36)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "jaccard_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_jaccard[model] = jaccard_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qwen-2.5-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen-2.5-7b-instruct\"\n",
    "# llm_companies = llm_ner(base_url=llm_vendors['bothub'],\n",
    "#                         api_key=os.getenv('BOT_HUB_KEY'),\n",
    "#                         model=model,\n",
    "#                         texts=texts)\n",
    "\n",
    "# save_ner_results(llm_companies, model)\n",
    "# grok_results = load_ner_results(model)\n",
    "# ### Lemmatization\n",
    "# grok_lemm = spacy_lem_companies(grok_results.values.tolist())\n",
    "# grok_lemm_ = clean_lemmatized(grok_lemm)\n",
    "# assert save_cleaned_results(grok_lemm_, model, \"spacy\"), \"File is not saved!\"\n",
    "\n",
    "### Jaccard distance\n",
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "llm_jaccard[model] = jaccard_metric.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3b-latest': np.float64(0.43),\n",
       " 'gemma-2-2b-it': np.float64(0.28),\n",
       " 'ministral-3b-instruct': np.float64(0.0),\n",
       " 'deepseek-r1-distill-qwen-1.5b': np.float64(0.13),\n",
       " 'llama-3.1-8b-instruct': np.float64(0.36),\n",
       " 'qwen-2.5-7b-instruct': np.float64(0.49)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(llm_jaccard, orient=\"index\", columns=[\"jaccard_metric\"]).to_csv(\n",
    "    Path(PROCESSED_DATA_PATH) / \"llm_jaccard.csv\", index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Middle**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-3.3-70b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [07:42<00:00,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"llama-3.3-70b-instruct\"\n",
    "llm_companies = llm_ner(\n",
    "    base_url=llm_vendors[\"bothub\"],\n",
    "    api_key=os.getenv(\"BOT_HUB_KEY\"),\n",
    "    model=model,\n",
    "    texts=texts,\n",
    ")\n",
    "\n",
    "save_ner_results(llm_companies, model)\n",
    "ner_results = load_ner_results(model)\n",
    "### Lemmatizationb\n",
    "ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "ner_lemm_ = clean_lemmatized(ner_lemm)\n",
    "assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\"\n",
    "\n",
    "### Jaccard distance\n",
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "llm_jaccard[model] = jaccard_metric.astype(float)\n",
    "pd.DataFrame.from_dict(llm_jaccard, orient=\"index\", columns=[\"jaccard_metric\"]).to_csv(\n",
    "    Path(PROCESSED_DATA_PATH) / \"llm_jaccard.csv\", index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ministral-3b-latest': np.float64(0.43),\n",
       " 'gemma-2-2b-it': np.float64(0.28),\n",
       " 'ministral-3b-instruct': np.float64(0.0),\n",
       " 'deepseek-r1-distill-qwen-1.5b': np.float64(0.13),\n",
       " 'llama-3.1-8b-instruct': np.float64(0.36),\n",
       " 'qwen-2.5-7b-instruct': np.float64(0.49),\n",
       " 'llama-3.3-70b-instruct': np.float64(0.46)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ministral-3b-2410',\n",
       " 'ministral-3b-latest',\n",
       " 'ministral-8b-2410',\n",
       " 'ministral-8b-latest',\n",
       " 'open-mistral-7b',\n",
       " 'mistral-tiny',\n",
       " 'mistral-tiny-2312',\n",
       " 'open-mistral-nemo',\n",
       " 'open-mistral-nemo-2407',\n",
       " 'mistral-tiny-2407',\n",
       " 'mistral-tiny-latest',\n",
       " 'open-mixtral-8x7b',\n",
       " 'mistral-small',\n",
       " 'mistral-small-2312',\n",
       " 'open-mixtral-8x22b',\n",
       " 'open-mixtral-8x22b-2404',\n",
       " 'mistral-small-2402',\n",
       " 'mistral-small-2409',\n",
       " 'mistral-medium-2312',\n",
       " 'mistral-medium',\n",
       " 'mistral-medium-latest',\n",
       " 'mistral-large-2402',\n",
       " 'mistral-large-2407',\n",
       " 'mistral-large-2411',\n",
       " 'mistral-large-latest',\n",
       " 'pixtral-large-2411',\n",
       " 'pixtral-large-latest',\n",
       " 'mistral-large-2502-15-1-rc2',\n",
       " 'mistral-large-pixtral-2411',\n",
       " 'codestral-2405',\n",
       " 'codestral-2501',\n",
       " 'codestral-latest',\n",
       " 'codestral-2412',\n",
       " 'codestral-2411-rc5',\n",
       " 'codestral-mamba-2407',\n",
       " 'open-codestral-mamba',\n",
       " 'codestral-mamba-latest',\n",
       " 'pixtral-12b-2409',\n",
       " 'pixtral-12b',\n",
       " 'pixtral-12b-latest',\n",
       " 'mistral-small-2501',\n",
       " 'mistral-small-latest',\n",
       " 'mistral-saba-2502',\n",
       " 'mistral-saba-latest',\n",
       " 'mistral-embed',\n",
       " 'mistral-moderation-2411',\n",
       " 'mistral-moderation-latest']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_service_models(llm_vendors[\"mistral\"], os.getenv(\"MISTRAL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [05:08<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"mistral-large-latest\"\n",
    "llm_companies = llm_ner(\n",
    "    base_url=llm_vendors[\"mistral\"],\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    model=model,\n",
    "    texts=texts,\n",
    ")\n",
    "\n",
    "save_ner_results(llm_companies, model)\n",
    "ner_results = load_ner_results(model)\n",
    "### Lemmatizationb\n",
    "ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "ner_lemm_ = clean_lemmatized(ner_lemm)\n",
    "assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\"\n",
    "\n",
    "### Jaccard distance\n",
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "llm_jaccard[model] = jaccard_metric.astype(float)\n",
    "pd.DataFrame.from_dict(llm_jaccard, orient=\"index\", columns=[\"jaccard_metric\"]).to_csv(\n",
    "    Path(PROCESSED_DATA_PATH) / \"llm_jaccard.csv\", index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-3.1-405b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [09:06<00:00,  3.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"llama-3.1-405b-instruct\"\n",
    "llm_companies = llm_ner(\n",
    "    base_url=llm_vendors[\"bothub\"],\n",
    "    api_key=os.getenv(\"BOT_HUB_KEY\"),\n",
    "    model=model,\n",
    "    texts=texts,\n",
    ")\n",
    "\n",
    "save_ner_results(llm_companies, model)\n",
    "ner_results = load_ner_results(model)\n",
    "### Lemmatizationb\n",
    "ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "ner_lemm_ = clean_lemmatized(ner_lemm)\n",
    "assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\"\n",
    "\n",
    "### Jaccard distance\n",
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "llm_jaccard[model] = jaccard_metric.astype(float)\n",
    "save_jaccard_results(llm_jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.52)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Large**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## claude-3.5-sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [10:55<00:00,  4.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"claude-3.5-sonnet\"\n",
    "llm_companies = llm_ner(\n",
    "    base_url=llm_vendors[\"bothub\"],\n",
    "    api_key=os.getenv(\"BOT_HUB_KEY\"),\n",
    "    model=model,\n",
    "    texts=texts,\n",
    ")\n",
    "\n",
    "save_ner_results(llm_companies, model)\n",
    "ner_results = load_ner_results(model)\n",
    "### Lemmatizationb\n",
    "ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "ner_lemm_ = clean_lemmatized(ner_lemm)\n",
    "assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\"\n",
    "\n",
    "### Jaccard distance\n",
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "llm_jaccard[model] = jaccard_metric.astype(float)\n",
    "save_jaccard_results(llm_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.63)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 25/149 [14:21<1:14:25, 36.01s/it]"
     ]
    }
   ],
   "source": [
    "model = \"deepseek-r1\"\n",
    "llm_companies = llm_ner(\n",
    "    base_url=llm_vendors[\"bothub\"],\n",
    "    api_key=os.getenv(\"BOT_HUB_KEY\"),\n",
    "    model=model,\n",
    "    texts=texts,\n",
    ")\n",
    "\n",
    "save_ner_results(llm_companies, model)\n",
    "ner_results = load_ner_results(model)\n",
    "### Lemmatizationb\n",
    "ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "ner_lemm_ = clean_lemmatized(ner_lemm)\n",
    "assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\"\n",
    "\n",
    "### Jaccard distance\n",
    "ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "llm_jaccard[model] = jaccard_metric.astype(float)\n",
    "save_jaccard_results(llm_jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o3-mini': np.float64(0.45)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = \"o3-mini\"\n",
    "# llm_companies = llm_ner(base_url=llm_vendors['bothub'],\n",
    "#                         api_key=os.getenv('BOT_HUB_KEY'),\n",
    "#                         model=model,\n",
    "#                         texts=texts)\n",
    "\n",
    "# save_ner_results(llm_companies, model)\n",
    "# ner_results = load_ner_results(model)\n",
    "# ### Lemmatizationb\n",
    "# ner_lemm = spacy_lem_companies(ner_results.values.tolist())\n",
    "# ner_lemm_ = clean_lemmatized(ner_lemm)\n",
    "# assert save_cleaned_results(ner_lemm_, model, \"spacy\"), \"File is not saved!\"\n",
    "\n",
    "# ### Jaccard distance\n",
    "# ner_lemm__ = load_cleaned_results(model, \"spacy\")\n",
    "# jaccard_metric = jaccard_feedbacks(ner_lemm__, labels_no_lemma)\n",
    "# llm_jaccard[model] = jaccard_metric.astype(float)\n",
    "save_jaccard_results(llm_jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_metric = pd.read_csv(Path(PROCESSED_DATA_PATH) / \"llm_jaccard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_metric.rename(columns={\"Unnamed: 0\": \"model\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {0: 'ministral-3b-latest',\n",
       "  1: 'gemma-2-2b-it',\n",
       "  2: 'ministral-3b-instruct',\n",
       "  3: 'deepseek-r1-distill-qwen-1.5b',\n",
       "  4: 'llama-3.1-8b-instruct',\n",
       "  5: 'qwen-2.5-7b-instruct',\n",
       "  6: 'llama-3.3-70b-instruct',\n",
       "  7: 'mistral-large-latest',\n",
       "  8: 'llama-3.1-405b-instruct',\n",
       "  9: 'claude-3.5-sonnet',\n",
       "  10: 'o3-mini'},\n",
       " 'jaccard_metric': {0: 0.43,\n",
       "  1: 0.28,\n",
       "  2: 0.0,\n",
       "  3: 0.13,\n",
       "  4: 0.36,\n",
       "  5: 0.49,\n",
       "  6: 0.46,\n",
       "  7: 0.47,\n",
       "  8: 0.52,\n",
       "  9: 0.63,\n",
       "  10: 0.45}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_metric.to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
